{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bac88216-cc3e-4e1d-87ca-845d9abab3a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: eng_Latn.devtest with 1012 sentences\n",
      "Loaded: hin_Deva.devtest with 1012 sentences\n",
      "Loaded: mar_Deva.devtest with 1012 sentences\n",
      "Loaded: guj_Gujr.devtest with 1012 sentences\n",
      "Loaded: ben_Beng.devtest with 1012 sentences\n",
      "Loaded: tel_Telu.devtest with 1012 sentences\n",
      "Loaded: tam_Taml.devtest with 1012 sentences\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def load_devtest_files(devtest_directory, specific_files):\n",
    "    all_data = {}\n",
    "\n",
    "    for file in specific_files:\n",
    "        file_path = os.path.join(devtest_directory, file)\n",
    "        \n",
    "        if os.path.exists(file_path):\n",
    "            try:\n",
    " \n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    sentences = f.readlines()\n",
    "                sentences = [sentence.strip() for sentence in sentences if sentence.strip()]  # Remove empty lines and strip whitespace\n",
    "\n",
    "                all_data[file] = sentences\n",
    "                print(f\"Loaded: {file} with {len(sentences)} sentences\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {file}: {e}\")\n",
    "        else:\n",
    "            print(f\"File not found: {file}\")\n",
    "\n",
    "    return all_data\n",
    "\n",
    "\n",
    "devtest_directory = r'D:\\Machine translation model\\flores200_dataset\\flores200_dataset\\devtest'\n",
    "specific_files = [\n",
    "    'eng_Latn.devtest',\n",
    "    'hin_Deva.devtest',\n",
    "    'mar_Deva.devtest',\n",
    "    'guj_Gujr.devtest',\n",
    "    'ben_Beng.devtest',\n",
    "    'tel_Telu.devtest',\n",
    "    'tam_Taml.devtest',\n",
    "]\n",
    "\n",
    "\n",
    "loaded_data = load_devtest_files(devtest_directory, specific_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "24a0baaf-20ae-48b0-8b69-8777ea514370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\Machine translation model\\Machine Translation Model\\OUTPUT MT\\parallel_dataset_eng_hin.xlsx with 150 entries.\n",
      "Saved: D:\\Machine translation model\\Machine Translation Model\\OUTPUT MT\\parallel_dataset_eng_mar.xlsx with 150 entries.\n",
      "Saved: D:\\Machine translation model\\Machine Translation Model\\OUTPUT MT\\parallel_dataset_eng_guj.xlsx with 150 entries.\n",
      "Saved: D:\\Machine translation model\\Machine Translation Model\\OUTPUT MT\\parallel_dataset_eng_ben.xlsx with 150 entries.\n",
      "Saved: D:\\Machine translation model\\Machine Translation Model\\OUTPUT MT\\parallel_dataset_eng_tel.xlsx with 150 entries.\n",
      "Saved: D:\\Machine translation model\\Machine Translation Model\\OUTPUT MT\\parallel_dataset_eng_tam.xlsx with 150 entries.\n"
     ]
    }
   ],
   "source": [
    "def save_random_samples_to_excel(loaded_data, sample_size=150):\n",
    "    english_sentences = loaded_data['eng_Latn.devtest']\n",
    "    \n",
    "    output_directory = r'D:\\Machine translation model\\Machine Translation Model\\OUTPUT MT'\n",
    "\n",
    "    sample_size = min(sample_size, len(english_sentences))\n",
    "\n",
    "    for lang_file, lang_sentences in loaded_data.items():\n",
    "        if lang_file != 'eng_Latn.devtest':\n",
    "    \n",
    "            min_len = min(len(english_sentences), len(lang_sentences))\n",
    "            if min_len < sample_size:\n",
    "                print(f\"Not enough sentences to sample for {lang_file}. Available: {min_len}\")\n",
    "                continue\n",
    "            \n",
    "            english_sample = pd.Series(english_sentences).sample(n=sample_size, random_state=42).reset_index(drop=True)\n",
    "            lang_sample = pd.Series(lang_sentences).sample(n=sample_size, random_state=42).reset_index(drop=True)\n",
    "\n",
    "            parallel_df = pd.DataFrame({\n",
    "                'English': english_sample,\n",
    "                'Translation': lang_sample\n",
    "            })\n",
    "\n",
    "            lang_code = lang_file.split('_')[0]\n",
    "            excel_filename = os.path.join(output_directory, f'parallel_dataset_eng_{lang_code}.xlsx')\n",
    "            parallel_df.to_excel(excel_filename, index=False)\n",
    "            print(f'Saved: {excel_filename} with {len(parallel_df)} entries.')\n",
    "save_random_samples_to_excel(loaded_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4545a7-1774-4ee4-b423-213f0d0215ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def create_parallel_datasets(languages, combined_df):\n",
    "    parallel_datasets = {}\n",
    "    for lang in languages:\n",
    "        eng_data = combined_df[combined_df['source_file'] == 'eng_Latn.devtest'].reset_index(drop=True)\n",
    "        lang_data = combined_df[combined_df['source_file'] == lang].reset_index(drop=True)\n",
    "        min_len = min(len(eng_data), len(lang_data))\n",
    "        eng_data = eng_data.iloc[:min_len]\n",
    "        lang_data = lang_data.iloc[:min_len]\n",
    "\n",
    "        parallel_df = pd.DataFrame({\n",
    "            'English': eng_data[0],\n",
    "            'Translation': lang_data[0] \n",
    "        })\n",
    "\n",
    "        parallel_df.dropna(inplace=True)\n",
    "\n",
    "        parallel_df = parallel_df.sample(n=min(150, len(parallel_df)), random_state=42)\n",
    "\n",
    "        parallel_datasets[lang] = parallel_df\n",
    "\n",
    "    return parallel_datasets\n",
    "\n",
    "languages = [\n",
    "    'hin_Deva.devtest',\n",
    "    'mar_Deva.devtest',\n",
    "    'guj_Gujr.devtest',\n",
    "    'ben_Beng.devtest',\n",
    "    'tel_Telu.devtest',\n",
    "    'tam_Taml.devtest'\n",
    "]\n",
    "\n",
    "parallel_datasets = create_parallel_datasets(languages, combined_df)\n",
    "\n",
    "for lang, df in parallel_datasets.items():\n",
    "    lang_code = lang.split('_')[0]\n",
    "    excel_filename = f'randomMT_eng_{lang_code}.xlsx'\n",
    "    df.to_excel(excel_filename, index=False)\n",
    "    print(f'Saved: {excel_filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "40daca15-7b6e-47d6-89f9-7d4acc249582",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = 'Your APi key'\n",
    "\n",
    "def evaluate_translation(english_sentence, target_language):\n",
    "    prompt = f\"Translate the following sentence to {target_language}:\\n\\n{english_sentence}\\n\\nTranslation:\"\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=60\n",
    "    )\n",
    "\n",
    "    return response['choices'][0]['message']['content'].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5d98ca-7cb5-44d1-96d9-24cf844645d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_parallel_datasets(loaded_data):\n",
    "    results = {}\n",
    "\n",
    "    for lang_file, lang_sentences in loaded_data.items():\n",
    "        if lang_file != 'eng_Latn.devtest':\n",
    "            lang_code = lang_file.split('_')[0]\n",
    "            results[lang_code] = []\n",
    "            for english_sentence, lang_translation in zip(loaded_data['eng_Latn.devtest'], lang_sentences):\n",
    "                evaluated_translation = evaluate_translation(english_sentence, lang_code)\n",
    "                results[lang_code].append({\n",
    "                    'English': english_sentence,\n",
    "                    'Translation': lang_translation,\n",
    "                    'GPT_Translation': evaluated_translation\n",
    "                })\n",
    "\n",
    "            print(f\"Evaluated translations for {lang_code}: {len(results[lang_code])} sentences.\")\n",
    "\n",
    "    return results\n",
    "evaluation_results = evaluate_parallel_datasets(loaded_data)\n",
    "\n",
    "\n",
    "def save_evaluation_results(evaluation_results):\n",
    "    for lang_code, evaluations in evaluation_results.items():\n",
    "        df = pd.DataFrame(evaluations)\n",
    "        excel_filename = f'evaluation_results_{lang_code}.xlsx'\n",
    "        df.to_excel(excel_filename, index=False)\n",
    "        print(f'Saved evaluation results to: {excel_filename}')\n",
    "\n",
    "# Save the evaluation results\n",
    "save_evaluation_results(evaluation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f578ee-9e14-4ecd-aeb4-5966330d07d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
